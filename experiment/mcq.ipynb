{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key = KEY,\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    temperature = 0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz of {number} multiple choice questions fro {subject} students in {tone} tone.\n",
    "Make sure the questions are not repeated and check all the questions to be confirming the text as well.\n",
    "Make sure to format your response like RESPONSE_JSON below and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    template=TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=quiz_generation_prompt,\n",
    "    output_key=\"quiz\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2=\"\"\"\n",
    "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
    "if the quiz is not at per with the cognitive and analytical abilities of the students,\\\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt = PromptTemplate(\n",
    "    input_variables=[\"subject, quiz\"],\n",
    "    template=TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=quiz_generation_prompt,\n",
    "    output_key=\"review\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain = SequentialChain(\n",
    "    chains=[quiz_chain, review_chain],\n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    output_variables=[\"quiz\", \"review\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as f:\n",
    "    TEXT = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# serialize python dictionary into a JSON-formatted string\n",
    "\n",
    "json.dumps(RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER = 5\n",
    "SUBJECT = \"RAG using Llama Index\"\n",
    "TONE = \"simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:Retrieval Augmented Generation (RAG)\n",
      "LLMs are trained on enormous bodies of data but they aren’t trained on your data. Retrieval-Augmented Generation (RAG) solves this problem by adding your data to the data LLMs already have access to. You will see references to RAG frequently in this documentation.\n",
      "\n",
      "In RAG, your data is loaded and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n",
      "\n",
      "Even if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application.\n",
      "\n",
      "\n",
      "\n",
      "Stages within RAG\n",
      "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
      "\n",
      "Loading: this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n",
      "\n",
      "Indexing: this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
      "\n",
      "Storing: once your data is indexed you will almost always want to store your index, as well as other metadata, to avoid having to re-index it.\n",
      "\n",
      "Querying: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
      "\n",
      "Evaluation: a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.\n",
      "\n",
      "\n",
      "\n",
      "Important concepts within each step\n",
      "There are also some terms you’ll encounter that refer to steps within each of these stages.\n",
      "\n",
      "Loading stage\n",
      "Nodes and Documents: A Document is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database. A Node is the atomic unit of data in LlamaIndex and represents a “chunk” of a source Document. Nodes have metadata that relate them to the document they are in and to other nodes.\n",
      "\n",
      "Connectors: A data connector (often called a Reader) ingests data from different data sources and data formats into Documents and Nodes.\n",
      "\n",
      "Indexing Stage\n",
      "Indexes: Once you’ve ingested your data, LlamaIndex will help you index the data into a structure that’s easy to retrieve. This usually involves generating vector embeddings which are stored in a specialized database called a vector store. Indexes can also store a variety of metadata about your data.\n",
      "\n",
      "Embeddings LLMs generate numerical representations of data called embeddings. When filtering your data for relevance, LlamaIndex will convert queries into embeddings, and your vector store will find data that is numerically similar to the embedding of your query.\n",
      "\n",
      "Querying Stage\n",
      "Retrievers: A retriever defines how to efficiently retrieve relevant context from an index when given a query. Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it’s done.\n",
      "\n",
      "Routers: A router determines which retriever will be used to retrieve relevant context from the knowledge base. More specifically, the RouterRetriever class, is responsible for selecting one or multiple candidate retrievers to execute a query. They use a selector to choose the best option based on each candidate’s metadata and the query.\n",
      "\n",
      "Node Postprocessors: A node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re-ranking logic to them.\n",
      "\n",
      "Response Synthesizers: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\n",
      "\n",
      "Putting it all together\n",
      "There are endless use cases for data-backed LLM applications but they can be roughly grouped into three categories:\n",
      "\n",
      "Query Engines: A query engine is an end-to-end pipeline that allows you to ask questions over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\n",
      "\n",
      "Chat Engines: A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question-and-answer).\n",
      "\n",
      "Agents: An agent is an automated decision-maker powered by an LLM that interacts with the world via a set of tools. Agents can take an arbitrary number of steps to complete a given task, dynamically deciding on the best course of action rather than following pre-determined steps. This gives it additional flexibility to tackle more complex tasks.\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz of 5 multiple choice questions fro RAG using Llama Index students in simple tone.\n",
      "Make sure the questions are not repeated and check all the questions to be confirming the text as well.\n",
      "Make sure to format your response like RESPONSE_JSON below and use it as a guide. Ensure to make 5 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:Retrieval Augmented Generation (RAG)\n",
      "LLMs are trained on enormous bodies of data but they aren’t trained on your data. Retrieval-Augmented Generation (RAG) solves this problem by adding your data to the data LLMs already have access to. You will see references to RAG frequently in this documentation.\n",
      "\n",
      "In RAG, your data is loaded and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n",
      "\n",
      "Even if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application.\n",
      "\n",
      "\n",
      "\n",
      "Stages within RAG\n",
      "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
      "\n",
      "Loading: this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n",
      "\n",
      "Indexing: this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
      "\n",
      "Storing: once your data is indexed you will almost always want to store your index, as well as other metadata, to avoid having to re-index it.\n",
      "\n",
      "Querying: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
      "\n",
      "Evaluation: a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.\n",
      "\n",
      "\n",
      "\n",
      "Important concepts within each step\n",
      "There are also some terms you’ll encounter that refer to steps within each of these stages.\n",
      "\n",
      "Loading stage\n",
      "Nodes and Documents: A Document is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database. A Node is the atomic unit of data in LlamaIndex and represents a “chunk” of a source Document. Nodes have metadata that relate them to the document they are in and to other nodes.\n",
      "\n",
      "Connectors: A data connector (often called a Reader) ingests data from different data sources and data formats into Documents and Nodes.\n",
      "\n",
      "Indexing Stage\n",
      "Indexes: Once you’ve ingested your data, LlamaIndex will help you index the data into a structure that’s easy to retrieve. This usually involves generating vector embeddings which are stored in a specialized database called a vector store. Indexes can also store a variety of metadata about your data.\n",
      "\n",
      "Embeddings LLMs generate numerical representations of data called embeddings. When filtering your data for relevance, LlamaIndex will convert queries into embeddings, and your vector store will find data that is numerically similar to the embedding of your query.\n",
      "\n",
      "Querying Stage\n",
      "Retrievers: A retriever defines how to efficiently retrieve relevant context from an index when given a query. Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it’s done.\n",
      "\n",
      "Routers: A router determines which retriever will be used to retrieve relevant context from the knowledge base. More specifically, the RouterRetriever class, is responsible for selecting one or multiple candidate retrievers to execute a query. They use a selector to choose the best option based on each candidate’s metadata and the query.\n",
      "\n",
      "Node Postprocessors: A node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re-ranking logic to them.\n",
      "\n",
      "Response Synthesizers: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\n",
      "\n",
      "Putting it all together\n",
      "There are endless use cases for data-backed LLM applications but they can be roughly grouped into three categories:\n",
      "\n",
      "Query Engines: A query engine is an end-to-end pipeline that allows you to ask questions over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\n",
      "\n",
      "Chat Engines: A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question-and-answer).\n",
      "\n",
      "Agents: An agent is an automated decision-maker powered by an LLM that interacts with the world via a set of tools. Agents can take an arbitrary number of steps to complete a given task, dynamically deciding on the best course of action rather than following pre-determined steps. This gives it additional flexibility to tackle more complex tasks.\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz of 5 multiple choice questions fro RAG using Llama Index students in simple tone.\n",
      "Make sure the questions are not repeated and check all the questions to be confirming the text as well.\n",
      "Make sure to format your response like RESPONSE_JSON below and use it as a guide. Ensure to make 5 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# How to setup token usage tracking in langchain\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    response = generate_evaluate_chain(\n",
    "        {\n",
    "            \"text\":TEXT,\n",
    "            \"number\":NUMBER,\n",
    "            \"subject\":SUBJECT,\n",
    "            \"tone\":TONE,\n",
    "            \"response_json\":json.dumps(RESPONSE_JSON),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens:3318\n",
      "Prompt Tokens:2460\n",
      "Completion Tokens:858\n",
      "Total Cost:0.005406\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Tokens:{cb.total_tokens}\")\n",
    "print(f\"Prompt Tokens:{cb.prompt_tokens}\")\n",
    "print(f\"Completion Tokens:{cb.completion_tokens}\")\n",
    "print(f\"Total Cost:{cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Retrieval Augmented Generation (RAG)\\nLLMs are trained on enormous bodies of data but they aren’t trained on your data. Retrieval-Augmented Generation (RAG) solves this problem by adding your data to the data LLMs already have access to. You will see references to RAG frequently in this documentation.\\n\\nIn RAG, your data is loaded and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\\n\\nEven if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application.\\n\\n\\n\\nStages within RAG\\nThere are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\\n\\nLoading: this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\\n\\nIndexing: this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\\n\\nStoring: once your data is indexed you will almost always want to store your index, as well as other metadata, to avoid having to re-index it.\\n\\nQuerying: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\\n\\nEvaluation: a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.\\n\\n\\n\\nImportant concepts within each step\\nThere are also some terms you’ll encounter that refer to steps within each of these stages.\\n\\nLoading stage\\nNodes and Documents: A Document is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database. A Node is the atomic unit of data in LlamaIndex and represents a “chunk” of a source Document. Nodes have metadata that relate them to the document they are in and to other nodes.\\n\\nConnectors: A data connector (often called a Reader) ingests data from different data sources and data formats into Documents and Nodes.\\n\\nIndexing Stage\\nIndexes: Once you’ve ingested your data, LlamaIndex will help you index the data into a structure that’s easy to retrieve. This usually involves generating vector embeddings which are stored in a specialized database called a vector store. Indexes can also store a variety of metadata about your data.\\n\\nEmbeddings LLMs generate numerical representations of data called embeddings. When filtering your data for relevance, LlamaIndex will convert queries into embeddings, and your vector store will find data that is numerically similar to the embedding of your query.\\n\\nQuerying Stage\\nRetrievers: A retriever defines how to efficiently retrieve relevant context from an index when given a query. Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it’s done.\\n\\nRouters: A router determines which retriever will be used to retrieve relevant context from the knowledge base. More specifically, the RouterRetriever class, is responsible for selecting one or multiple candidate retrievers to execute a query. They use a selector to choose the best option based on each candidate’s metadata and the query.\\n\\nNode Postprocessors: A node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re-ranking logic to them.\\n\\nResponse Synthesizers: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.\\n\\nPutting it all together\\nThere are endless use cases for data-backed LLM applications but they can be roughly grouped into three categories:\\n\\nQuery Engines: A query engine is an end-to-end pipeline that allows you to ask questions over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\\n\\nChat Engines: A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question-and-answer).\\n\\nAgents: An agent is an automated decision-maker powered by an LLM that interacts with the world via a set of tools. Agents can take an arbitrary number of steps to complete a given task, dynamically deciding on the best course of action rather than following pre-determined steps. This gives it additional flexibility to tackle more complex tasks.',\n",
       " 'number': 5,\n",
       " 'subject': 'RAG using Llama Index',\n",
       " 'tone': 'simple',\n",
       " 'response_json': '{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}',\n",
       " 'quiz': '{\"1\": {\"mcq\": \"What is the purpose of Retrieval-Augmented Generation (RAG)?\", \"options\": {\"a\": \"To train LLMs on enormous bodies of data\", \"b\": \"To add user data to the data LLMs already have access to\", \"c\": \"To create vector embeddings for indexing data\", \"d\": \"To store and retrieve data efficiently\"}, \"correct\": \"b\"}, \\n\"2\": {\"mcq\": \"What is the indexing stage in RAG?\", \"options\": {\"a\": \"Loading data from different sources\", \"b\": \"Creating a data structure for querying the data\", \"c\": \"Storing the indexed data\", \"d\": \"Evaluating the effectiveness of the pipeline\"}, \"correct\": \"b\"}, \\n\"3\": {\"mcq\": \"What are retrievers used for in the querying stage of RAG?\", \"options\": {\"a\": \"To generate vector embeddings for data\", \"b\": \"To determine the best retrieval strategy\", \"c\": \"To apply transformations to retrieved nodes\", \"d\": \"To generate responses from an LLM\"}, \"correct\": \"b\"}, \\n\"4\": {\"mcq\": \"What is the purpose of a chat engine in RAG?\", \"options\": {\"a\": \"To ask questions over the data\", \"b\": \"To have a conversation with the data\", \"c\": \"To interact with the world via a set of tools\", \"d\": \"To dynamically decide on the best course of action\"}, \"correct\": \"b\"}, \\n\"5\": {\"mcq\": \"What is the main advantage of using an agent in RAG?\", \"options\": {\"a\": \"It can train LLMs on enormous bodies of data\", \"b\": \"It can store and retrieve data efficiently\", \"c\": \"It can have a conversation with the data\", \"d\": \"It can dynamically decide on the best course of action\"}, \"correct\": \"d\"}}',\n",
       " 'review': '{\"1\": {\"mcq\": \"What is the purpose of Retrieval-Augmented Generation (RAG)?\", \"options\": {\"a\": \"To train LLMs on enormous bodies of data\", \"b\": \"To add user data to the data LLMs already have access to\", \"c\": \"To create vector embeddings for querying data\", \"d\": \"To store and index data for efficient retrieval\"}, \"correct\": \"b\"}, \\n\"2\": {\"mcq\": \"What is the indexing stage in RAG?\", \"options\": {\"a\": \"Creating vector embeddings\", \"b\": \"Storing data in a specialized database\", \"c\": \"Retrieving relevant context from the knowledge base\", \"d\": \"Converting queries into embeddings\"}, \"correct\": \"b\"}, \\n\"3\": {\"mcq\": \"What does a retriever do in RAG?\", \"options\": {\"a\": \"Generates numerical representations of data\", \"b\": \"Efficiently retrieves relevant context from an index\", \"c\": \"Applies transformations or re-ranking logic to retrieved nodes\", \"d\": \"Generates a response from an LLM\"}, \"correct\": \"b\"}, \\n\"4\": {\"mcq\": \"What is the purpose of a router in RAG?\", \"options\": {\"a\": \"To generate vector embeddings\", \"b\": \"To select the best retriever for executing a query\", \"c\": \"To filter or re-rank retrieved nodes\", \"d\": \"To generate a response from an LLM\"}, \"correct\": \"b\"}, \\n\"5\": {\"mcq\": \"What is the main difference between a chat engine and an agent in RAG?\", \"options\": {\"a\": \"A chat engine allows for multiple back-and-forth conversations, while an agent follows pre-determined steps\", \"b\": \"A chat engine interacts with the world via a set of tools, while an agent takes in natural language queries\", \"c\": \"A chat engine is an end-to-end pipeline, while an agent is an automated decision-maker\", \"d\": \"A chat engine can tackle more complex tasks, while an agent can only answer questions\"}, \"correct\": \"a\"}}'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz = response.get(\"quiz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz = json.loads(quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_table_data = []\n",
    "for key, value in quiz.items():\n",
    "    mcq = value[\"mcq\"]\n",
    "    options = \" | \".join(\n",
    "        [\n",
    "            f\"{option} : {option_value}\"\n",
    "            for option, option_value in value[\"options\"].items()\n",
    "        ]\n",
    "    )\n",
    "    correct = value[\"correct\"]\n",
    "    quiz_table_data.append(\n",
    "\n",
    "       { \"MCQ\":mcq,\n",
    "        \"Choces\":options,\n",
    "        \"Correct\": correct\n",
    "}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'MCQ': 'What is the purpose of Retrieval-Augmented Generation (RAG)?',\n",
       "  'Choces': 'a : To train LLMs on enormous bodies of data | b : To add user data to the data LLMs already have access to | c : To create vector embeddings for indexing data | d : To store and retrieve data efficiently',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'What is the indexing stage in RAG?',\n",
       "  'Choces': 'a : Loading data from different sources | b : Creating a data structure for querying the data | c : Storing the indexed data | d : Evaluating the effectiveness of the pipeline',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'What are retrievers used for in the querying stage of RAG?',\n",
       "  'Choces': 'a : To generate vector embeddings for data | b : To determine the best retrieval strategy | c : To apply transformations to retrieved nodes | d : To generate responses from an LLM',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'What is the purpose of a chat engine in RAG?',\n",
       "  'Choces': 'a : To ask questions over the data | b : To have a conversation with the data | c : To interact with the world via a set of tools | d : To dynamically decide on the best course of action',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'What is the main advantage of using an agent in RAG?',\n",
       "  'Choces': 'a : It can train LLMs on enormous bodies of data | b : It can store and retrieve data efficiently | c : It can have a conversation with the data | d : It can dynamically decide on the best course of action',\n",
       "  'Correct': 'd'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_df = pd.DataFrame(quiz_table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCQ</th>\n",
       "      <th>Choces</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the purpose of Retrieval-Augmented Gen...</td>\n",
       "      <td>a : To train LLMs on enormous bodies of data |...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the indexing stage in RAG?</td>\n",
       "      <td>a : Loading data from different sources | b : ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are retrievers used for in the querying s...</td>\n",
       "      <td>a : To generate vector embeddings for data | b...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the purpose of a chat engine in RAG?</td>\n",
       "      <td>a : To ask questions over the data | b : To ha...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the main advantage of using an agent i...</td>\n",
       "      <td>a : It can train LLMs on enormous bodies of da...</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 MCQ  \\\n",
       "0  What is the purpose of Retrieval-Augmented Gen...   \n",
       "1                 What is the indexing stage in RAG?   \n",
       "2  What are retrievers used for in the querying s...   \n",
       "3       What is the purpose of a chat engine in RAG?   \n",
       "4  What is the main advantage of using an agent i...   \n",
       "\n",
       "                                              Choces Correct  \n",
       "0  a : To train LLMs on enormous bodies of data |...       b  \n",
       "1  a : Loading data from different sources | b : ...       b  \n",
       "2  a : To generate vector embeddings for data | b...       b  \n",
       "3  a : To ask questions over the data | b : To ha...       b  \n",
       "4  a : It can train LLMs on enormous bodies of da...       d  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_df.to_csv(\"RAG.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12_30_2023_17_46_50.log'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
